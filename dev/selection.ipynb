{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dirpath_data = pathlib.Path(\"../data\")\n",
    "dirpath_docs = dirpath_data / \"docs\"\n",
    "dirpath_splits = dirpath_data / \"splits\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "The dataset used for this task is the `Question-Answer Dataset` with origins<br />\n",
    "from the paper [2], which originally followed a structure as described by the<br />\n",
    "the kaggle dataset page [1]:\n",
    "> \"The `question_answer_pairs.txt` files contain both the questions and answers.<br />\n",
    "> The columns in this file are as follows:<br />\n",
    "> * `ArticleTitle` is the name of the Wikipedia article from which questions and answers initially came<br />\n",
    "> * `Question` is the question<br />\n",
    "> * `Answer` is the answer<br />\n",
    "> * `DifficultyFromQuestioner` is the prescribed difficulty rating for the question as given to the question-writer<br />\n",
    "> * `DifficultyFromAnswerer` is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4<br />\n",
    "> * `ArticleFile` is the name of the file with the relevant article\"<br />\n",
    "\n",
    "This structure is, however, not the most convenient for consumption. For that<br />\n",
    "reason the [./etl.ipynb](ETL) process was designed to transform the data into a<br />\n",
    "simpler format.\n",
    "\n",
    "## Data within the `./data/docs` folder\n",
    "\n",
    "The documents that must be used in the Question Answering system are stored within<br />\n",
    "the `./data/docs` folder. Each document is a plain text file with the content of<br />\n",
    "the document that must be taken in consideration for the Question Answering task.\n",
    "\n",
    "## Data within the `./data/splits` folder\n",
    "\n",
    "Within the `./data/splits` folder there are 2 files: `train.csv` and `test.csv`.<br />\n",
    "which are CSVs cleaned and transformed from the original `question_answer_pairs.txt`<br />\n",
    "with the columns:\n",
    "\n",
    "* `question`: The question to be answered\n",
    "* `answer`: The answer to the question, used as ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(dirpath_splits / f\"{split_name}.csv\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"train\").head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Given the `RagQA` service does not require any pre-processing (e.g.: tokenisation)<br />\n",
    "of the text prior to inference or training, text is kept in its original form.\n",
    "\n",
    "In order to reduce variance caused by situations where the casing of words is<br />\n",
    "different or inconsistent, all text is converted to lowercase. Also data points<br />\n",
    "presenting null values are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna().copy()\n",
    "    df[\"question\"] = df.question.map(str.lower)\n",
    "    df[\"answer\"] = df.answer.map(str.lower)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = prepare_datset(load_dataset(\"train\"))\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Criteria\n",
    "\n",
    "**Target Metric**: Accuracy Score\n",
    "\n",
    "Due to their natural language nature, answering produced by the RAG cannot be<br />\n",
    "evaluated at a token-level. For that reason, it would require either a person<br />\n",
    "to evaluate it, or something with near-human level of judgement over text to<br />\n",
    "check whether the answers make sense. To solve this problem, we will use the<br />\n",
    "\"Automatic eValuation Approach to Question Answering Systems\" (AVA) proposed by [3].\n",
    "\n",
    "In terms of target metric, there are no categories or classes. Each Question may<br />\n",
    "either be (a) answered correctly, (b) answered incorrectly. Each answer is evaluated<br />\n",
    "semantically, not at the token level. For that reason, we will use the Accuracy<br />\n",
    "score which provides a ratio of correct answers over the total number of questions.\n",
    "\n",
    "`acc_score = len(corrects) / len(questions)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def is_answer_correct(\n",
    "    y_true: str, y_pred: str, entailmanet_label: str = \"entailment\", return_answer: bool = False\n",
    ") -> bool:\n",
    "    instructions = \"\\n\".join(\n",
    "        [\n",
    "            \"You are a semantic similarity analyst.\",\n",
    "            \"\",\n",
    "            \"You will read a `premise`.\",\n",
    "            \"then you will read a `hypothesis`.\",\n",
    "            \"then you will write a `label`\"\n",
    "            f\"- `{entailmanet_label}`: if `premise` and `hypothesis` could mean the same thing,\",\n",
    "            \"- `different`: otherwise\",\n",
    "            \"Ensure that your answer is the shortest possible, with no extra format or characters.\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job = \"\\n\".join([f\"premise=```{y_true}```\", f\"hypothesis=```{y_pred}```\", \"label=\"])\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": instructions}, {\"role\": \"user\", \"content\": job}]\n",
    "    )\n",
    "\n",
    "    label = completion.choices[0].message.content\n",
    "    correct = entailmanet_label in label.lower()\n",
    "    return correct if not return_answer else (correct, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    (y_true, y_pred, is_answer_correct(y_true, y_pred, return_answer=True))\n",
    "    for (y_true, y_pred) in [(\"yes\", \"affirmative\"), (\"yes\", \"negative\"), (\"no\", \"negative\")]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Protocol\n",
    "\n",
    "For evaluation a separate `test` split will be used. This approach is also known<br />\n",
    "as **Hold-out Evaluation**, and this split is based on the `rtatman/questionanswer-dataset`<br />\n",
    "dataset[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hlm12rag.training import QATrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = prepare_datset(load_dataset(\"test\"))\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QATrainer(dataset=df_train, correctness_fn=is_answer_correct)\n",
    "trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hlm12rag.modelling import RagQABuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_qa_model = RagQABuilder(dirpath=dirpath_docs).build()\n",
    "baseline_qa_out = trainer.train(model=baseline_qa_model)\n",
    "baseline_qa_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "```\n",
    "[1] Smith, N.A., Heilman, M., Hwa, R. 2008. Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge, Online, Source:https://www.cs.cmu.edu/~nasmith/papers/smith+heilman+hwa.nsf08.pdf\n",
    "\n",
    "[2] Tatman, R. 2018. The Question-Answer Dataset. https://www.kaggle.com/rtatman/questionanswer-dataset\n",
    "\n",
    "[3] Thuy Vu and Alessandro Moschitti. 2021. AVA: an Automatic eValuation Approach for Question Answering Systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online, 5223–5233. DOI:https://doi.org/10.18653/v1/2021.naacl-main.412\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
