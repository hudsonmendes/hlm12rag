{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dirpath_data = pathlib.Path(\"../data\")\n",
    "dirpath_docs = dirpath_data / \"docs\"\n",
    "dirpath_splits = dirpath_data / \"splits\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "The dataset used for this task is the `Question-Answer Dataset` with origins<br />\n",
    "from the paper [2], which originally followed a structure as described by the<br />\n",
    "the kaggle dataset page [1]:\n",
    "> \"The `question_answer_pairs.txt` files contain both the questions and answers.<br />\n",
    "> The columns in this file are as follows:<br />\n",
    "> * `ArticleTitle` is the name of the Wikipedia article from which questions and answers initially came<br />\n",
    "> * `Question` is the question<br />\n",
    "> * `Answer` is the answer<br />\n",
    "> * `DifficultyFromQuestioner` is the prescribed difficulty rating for the question as given to the question-writer<br />\n",
    "> * `DifficultyFromAnswerer` is a difficulty rating assigned by the individual who evaluated and answered the question, which may differ from the difficulty in field 4<br />\n",
    "> * `ArticleFile` is the name of the file with the relevant article\"<br />\n",
    "\n",
    "This structure is, however, not the most convenient for consumption. For that<br />\n",
    "reason the [./etl.ipynb](ETL) process was designed to transform the data into a<br />\n",
    "simpler format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data within the `./data/docs` folder\n",
    "\n",
    "The documents that must be used in the Question Answering system are stored within<br />\n",
    "the `./data/docs` folder. Each document is a plain text file with the content of<br />\n",
    "the document that must be taken in consideration for the Question Answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S08_set1_a1.txt  S08_set4_a8.txt  S09_set4_a6.txt  S10_set3_a4.txt\n",
      "S08_set1_a10.txt S08_set4_a9.txt  S09_set4_a7.txt  S10_set3_a5.txt\n",
      "S08_set1_a2.txt  S09_set1_a1.txt  S09_set4_a8.txt  S10_set3_a6.txt\n",
      "S08_set1_a3.txt  S09_set1_a10.txt S09_set4_a9.txt  S10_set3_a7.txt\n",
      "S08_set1_a4.txt  S09_set1_a2.txt  S09_set5_a1.txt  S10_set3_a8.txt\n",
      "S08_set1_a5.txt  S09_set1_a3.txt  S09_set5_a10.txt S10_set3_a9.txt\n",
      "S08_set1_a6.txt  S09_set1_a4.txt  S09_set5_a2.txt  S10_set4_a1.txt\n",
      "S08_set1_a7.txt  S09_set1_a5.txt  S09_set5_a3.txt  S10_set4_a10.txt\n",
      "S08_set1_a8.txt  S09_set1_a6.txt  S09_set5_a4.txt  S10_set4_a2.txt\n",
      "S08_set1_a9.txt  S09_set1_a7.txt  S09_set5_a5.txt  S10_set4_a3.txt\n",
      "S08_set2_a1.txt  S09_set1_a8.txt  S09_set5_a6.txt  S10_set4_a4.txt\n",
      "S08_set2_a10.txt S09_set1_a9.txt  S09_set5_a7.txt  S10_set4_a5.txt\n",
      "S08_set2_a2.txt  S09_set2_a1.txt  S09_set5_a8.txt  S10_set4_a6.txt\n",
      "S08_set2_a3.txt  S09_set2_a10.txt S09_set5_a9.txt  S10_set4_a7.txt\n",
      "S08_set2_a4.txt  S09_set2_a2.txt  S10_set1_a1.txt  S10_set4_a8.txt\n",
      "S08_set2_a5.txt  S09_set2_a3.txt  S10_set1_a10.txt S10_set4_a9.txt\n",
      "S08_set2_a6.txt  S09_set2_a4.txt  S10_set1_a2.txt  S10_set5_a1.txt\n",
      "S08_set2_a7.txt  S09_set2_a5.txt  S10_set1_a3.txt  S10_set5_a10.txt\n",
      "S08_set2_a8.txt  S09_set2_a6.txt  S10_set1_a4.txt  S10_set5_a2.txt\n",
      "S08_set2_a9.txt  S09_set2_a7.txt  S10_set1_a5.txt  S10_set5_a3.txt\n",
      "S08_set3_a1.txt  S09_set2_a8.txt  S10_set1_a6.txt  S10_set5_a4.txt\n",
      "S08_set3_a10.txt S09_set2_a9.txt  S10_set1_a7.txt  S10_set5_a5.txt\n",
      "S08_set3_a2.txt  S09_set3_a1.txt  S10_set1_a8.txt  S10_set5_a6.txt\n",
      "S08_set3_a3.txt  S09_set3_a10.txt S10_set1_a9.txt  S10_set5_a7.txt\n",
      "S08_set3_a4.txt  S09_set3_a2.txt  S10_set2_a1.txt  S10_set5_a8.txt\n",
      "S08_set3_a5.txt  S09_set3_a3.txt  S10_set2_a10.txt S10_set5_a9.txt\n",
      "S08_set3_a6.txt  S09_set3_a4.txt  S10_set2_a2.txt  S10_set6_a1.txt\n",
      "S08_set3_a7.txt  S09_set3_a5.txt  S10_set2_a3.txt  S10_set6_a10.txt\n",
      "S08_set3_a8.txt  S09_set3_a6.txt  S10_set2_a4.txt  S10_set6_a2.txt\n",
      "S08_set3_a9.txt  S09_set3_a7.txt  S10_set2_a5.txt  S10_set6_a3.txt\n",
      "S08_set4_a1.txt  S09_set3_a8.txt  S10_set2_a6.txt  S10_set6_a4.txt\n",
      "S08_set4_a10.txt S09_set3_a9.txt  S10_set2_a7.txt  S10_set6_a5.txt\n",
      "S08_set4_a2.txt  S09_set4_a1.txt  S10_set2_a8.txt  S10_set6_a6.txt\n",
      "S08_set4_a3.txt  S09_set4_a10.txt S10_set2_a9.txt  S10_set6_a7.txt\n",
      "S08_set4_a4.txt  S09_set4_a2.txt  S10_set3_a1.txt  S10_set6_a8.txt\n",
      "S08_set4_a5.txt  S09_set4_a3.txt  S10_set3_a10.txt S10_set6_a9.txt\n",
      "S08_set4_a6.txt  S09_set4_a4.txt  S10_set3_a2.txt\n",
      "S08_set4_a7.txt  S09_set4_a5.txt  S10_set3_a3.txt\n"
     ]
    }
   ],
   "source": [
    "!ls $dirpath_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data within the `./data/splits` folder\n",
    "\n",
    "Within the `./data/splits` folder there are 2 files: `train.csv` and `test.csv`.<br />\n",
    "which are CSVs cleaned and transformed from the original `question_answer_pairs.txt`<br />\n",
    "with the columns:\n",
    "\n",
    "* `question`: The question to be answered\n",
    "* `answer`: The answer to the question, used as ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(dirpath_splits / f\"{split_name}.csv\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does Henri Becquerel do?</td>\n",
       "      <td>was a physisist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where was Grover Cleveland married?</td>\n",
       "      <td>in the Blue Room in the White House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Was James Watt a member of the Lunar Society?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is London the capital of the United Kingdom?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On average, are cougar males heavier than fema...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                      What does Henri Becquerel do?   \n",
       "1                Where was Grover Cleveland married?   \n",
       "2      Was James Watt a member of the Lunar Society?   \n",
       "3       Is London the capital of the United Kingdom?   \n",
       "4  On average, are cougar males heavier than fema...   \n",
       "\n",
       "                                answer  \n",
       "0                      was a physisist  \n",
       "1  in the Blue Room in the White House  \n",
       "2                                  NaN  \n",
       "3                                  Yes  \n",
       "4                                  yes  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"train\").head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Given the `RagQA` service does not require any pre-processing (e.g.: tokenisation)<br />\n",
    "of the text prior to inference or training, text is kept in its original form.\n",
    "\n",
    "In order to reduce variance caused by situations where the casing of words is<br />\n",
    "different or inconsistent, all text is converted to lowercase. Also data points<br />\n",
    "presenting null values are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna().copy()\n",
    "    df[\"question\"] = df.question.map(str.lower)\n",
    "    df[\"answer\"] = df.answer.map(str.lower)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what does henri becquerel do?</td>\n",
       "      <td>was a physisist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where was grover cleveland married?</td>\n",
       "      <td>in the blue room in the white house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is london the capital of the united kingdom?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on average, are cougar males heavier than fema...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is polar bear a carnivore?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>&amp;#20415;&amp;#24403;/&amp;#20415;&amp;#30070; lunchbox or...</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>when does a flute produce sound?</td>\n",
       "      <td>when a stream of air directed across a hole in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>did volta marry before he became professor of ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>is an otter 's den called a holt ?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>was adams an opponent of the stamp act?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0                         what does henri becquerel do?   \n",
       "1                   where was grover cleveland married?   \n",
       "3          is london the capital of the united kingdom?   \n",
       "4     on average, are cougar males heavier than fema...   \n",
       "5                            is polar bear a carnivore?   \n",
       "...                                                 ...   \n",
       "3195  &#20415;&#24403;/&#20415;&#30070; lunchbox or...   \n",
       "3196                   when does a flute produce sound?   \n",
       "3197  did volta marry before he became professor of ...   \n",
       "3198                 is an otter 's den called a holt ?   \n",
       "3199            was adams an opponent of the stamp act?   \n",
       "\n",
       "                                                 answer  \n",
       "0                                       was a physisist  \n",
       "1                   in the blue room in the white house  \n",
       "3                                                   yes  \n",
       "4                                                   yes  \n",
       "5                                                   yes  \n",
       "...                                                 ...  \n",
       "3195                                           japanese  \n",
       "3196  when a stream of air directed across a hole in...  \n",
       "3197                                                 no  \n",
       "3198                                                yes  \n",
       "3199                                                yes  \n",
       "\n",
       "[2734 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = prepare_datset(load_dataset(\"train\"))\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Criteria\n",
    "\n",
    "**Target Metric**: Accuracy Score\n",
    "\n",
    "Due to their natural language nature, answering produced by the RAG cannot be<br />\n",
    "evaluated at a token-level. For that reason, it would require either a person<br />\n",
    "to evaluate it, or something with near-human level of judgement over text to<br />\n",
    "check whether the answers make sense. To solve this problem, we will use the<br />\n",
    "\"Automatic eValuation Approach to Question Answering Systems\" (AVA) proposed by [3].\n",
    "\n",
    "In terms of target metric, there are no categories or classes. Each Question may<br />\n",
    "either be (a) answered correctly, (b) answered incorrectly. Each answer is evaluated<br />\n",
    "semantically, not at the token level. For that reason, we will use the Accuracy<br />\n",
    "score which provides a ratio of correct answers over the total number of questions.\n",
    "\n",
    "`acc_score = len(corrects) / len(questions)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def is_answer_correct(\n",
    "    y_true: str,\n",
    "    y_pred: str,\n",
    "    entailmanet_label: str = \"entailment\",\n",
    "    return_answer: bool = False,\n",
    ") -> bool:\n",
    "    instructions = \"\\n\".join(\n",
    "        [\n",
    "            \"You are a semantic similarity analyst.\",\n",
    "            \"\",\n",
    "            \"You will read a `premise`.\",\n",
    "            \"then you will read a `hypothesis`.\",\n",
    "            \"then you will write a `label`\"\n",
    "            f\"- `{entailmanet_label}`: if `premise` and `hypothesis` could mean the same thing,\",\n",
    "            \"- `different`: otherwise\",\n",
    "            \"Ensure that your answer is the shortest possible, with no extra format or characters.\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job = \"\\n\".join([f\"premise=```{y_true}```\", f\"hypothesis=```{y_pred}```\", \"label=\"])\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": job},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    label = completion.choices[0].message.content\n",
    "    correct = entailmanet_label in label.lower()\n",
    "    return correct if not return_answer else (correct, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    (y_true, y_pred, is_answer_correct(y_true, y_pred, return_answer=True))\n",
    "    for (y_true, y_pred) in [\n",
    "        (\"yes\", \"affirmative\"),\n",
    "        (\"yes\", \"negative\"),\n",
    "        (\"no\", \"negative\"),\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Protocol\n",
    "\n",
    "For evaluation a separate `test` split will be used. This approach is also known<br />\n",
    "as **Hold-out Evaluation**, and this split is based on the `rtatman/questionanswer-dataset`<br />\n",
    "dataset[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hlm12rag.training import QATrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = prepare_datset(load_dataset(\"test\"))\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QATrainer(dataset=df_train, correctness_fn=is_answer_correct)\n",
    "trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hlm12rag.modelling import RagQABuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_qa_model = RagQABuilder(dirpath=dirpath_docs).build()\n",
    "baseline_qa_out = trainer.train(model=baseline_qa_model)\n",
    "baseline_qa_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "```\n",
    "[1] Smith, N.A., Heilman, M., Hwa, R. 2008. Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge, Online, Source:https://www.cs.cmu.edu/~nasmith/papers/smith+heilman+hwa.nsf08.pdf\n",
    "\n",
    "[2] Tatman, R. 2018. The Question-Answer Dataset. https://www.kaggle.com/rtatman/questionanswer-dataset\n",
    "\n",
    "[3] Thuy Vu and Alessandro Moschitti. 2021. AVA: an Automatic eValuation Approach for Question Answering Systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Online, 5223–5233. DOI:https://doi.org/10.18653/v1/2021.naacl-main.412\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
